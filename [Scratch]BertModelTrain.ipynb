{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 토큰 개수 30.0\n"
     ]
    }
   ],
   "source": [
    "sentences = pd.read_csv('./data/bookraw_list.txt')\n",
    "sentence_list = sentences.iloc[:,0]\n",
    "\n",
    "# nan 제거\n",
    "sentence_list = sentence_list[sentence_list.isna() == False].values\n",
    "\n",
    "# to numpy\n",
    "\n",
    "### token 상위 99% 이상인 문장 제거 \n",
    "num_sen = [len(sen.split(' ')) for sen in sentence_list.tolist()]\n",
    "\n",
    "percentile_99 = np.quantile(num_sen,0.99)\n",
    "\n",
    "sentence_list_under_99 = sentence_list[np.array(num_sen) < percentile_99 ].tolist()\n",
    "\n",
    "print(f\"최대 토큰 개수 {percentile_99}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'python', '##과', 'javascript', '##를', '만들며', '고민', '##했다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer 불러오기\n",
    "## 한글을 불러올 땐 strip accents = False 필수\n",
    "\n",
    "vocab = './data/vocab.txt'\n",
    "tokenizer = BertWordPieceTokenizer.from_file(vocab=vocab,strip_accents=False)\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode('python과 javascript를 만들며 고민했다')\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask_token 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) 문장 변형은 Token 중 15%\n",
    "# 2) 15% 변형 중 80%는 mask, 20%는 단어 바꾸기임.\n",
    "import random\n",
    "\n",
    "def mask_sentence(sentence : list, ids : list ,tokenizer, mask_rate = 0.15) :\n",
    "    '''\n",
    "    sentence : ['[CLS]', '피벗', '테이블에', '원본', '데이터의', '변경된', '값', '적용하기', '[SEP]']\n",
    "    ids : [2, 4537, 11429, 7412, 4081, 12359, 84, 3604, 3]\n",
    "    '''\n",
    "\n",
    "    sentence = sentence.copy()\n",
    "    ids = ids.copy()\n",
    "\n",
    "    # token 개수\n",
    "    len_sen = len(sentence)\n",
    "\n",
    "    # token_mask \n",
    "    token_mask = [True for _ in range(len_sen)]\n",
    "\n",
    "    # -2 하는 이유 [CLS], [SEP] 제외\n",
    "    mask_amount = round(len_sen-2 * mask_rate)\n",
    "\n",
    "    for _ in range(mask_amount) :\n",
    "        i = random.randint(1,len_sen-2)\n",
    "\n",
    "        if random.random() < 0.8 :\n",
    "            sentence[i] ='[MASK]'\n",
    "            ids[i] = 4\n",
    "        else : \n",
    "            # 5부터 선정하는 이유 \n",
    "            # [PAD] = 0, [UNK] = 1, [CLS] = 2, [SEP] = 3, [MASK] = 4\n",
    "\n",
    "            j = random.randint(5,tokenizer.get_vocab_size())\n",
    "            sentence[i] = tokenizer.decode([j])\n",
    "            ids[i] = j\n",
    "\n",
    "        token_mask[i] = False\n",
    "\n",
    "        return sentence, ids ,token_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def padding_sentence(mask_sentence: tuple,max_length = 40) :\n",
    "    sentence, ids ,token_mask = mask_sentence\n",
    "    len_sen = len(sentence)\n",
    "    sentence.extend(['[PAD]' for _ in range(max_length-len_sen)])\n",
    "    ids.extend([0 for _ in range(max_length-len_sen)])\n",
    "    token_mask.extend([False for _ in range(max_length-len_sen)])\n",
    "\n",
    "    return sentence, ids ,token_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token 구현한 것 실현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['[CLS]', '피벗', '테이블에', '원본', '데이터의', '변경된', '[MASK]', '적용하기', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], [2, 4537, 11429, 7412, 4081, 12359, 4, 3604, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [True, True, True, True, True, True, False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "max_length = 40 \n",
    "\n",
    "def tokenizing_sentence(sent:str, max_length) : \n",
    "\n",
    "    encoded = tokenizer.encode(sent)\n",
    "    \n",
    "    sen_token = encoded.tokens\n",
    "    sen_ids = encoded.ids\n",
    "    \n",
    "    if len(sen_token) > max_length : \n",
    "        max_length = len(sen_token)\n",
    "\n",
    "    # semtemce, ids, token_mask\n",
    "    mss = mask_sentence(sen_token,sen_ids,tokenizer=tokenizer)\n",
    "\n",
    "    # 패딩 구현\n",
    "    sentence, ids ,token_mask = padding_sentence(mss)\n",
    "        \n",
    "    return sentence, ids ,token_mask\n",
    "\n",
    "print(tokenizing_sentence(sent,max_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSP\n",
    "\n",
    "NSP를 먼저 만든 다음\n",
    "\n",
    "그 안에서 MLM을 구성하는게 맞다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_toc      ['서문', '베타 리더 추천사', '이 책의 구성', '장 데이터 사이언티스트 이...\n",
      "book_intro    ['', '데이터 사이언티스트의 실제 업무를 알려드리고 데이터 사이언티스트가 되기 ...\n",
      "publisher     ['데이터 사이언티스트가 되기 위해서라면 가장 먼저 풀어야 할 데이터 사이언티스트의...\n",
      "Name: 5067, dtype: object\n"
     ]
    }
   ],
   "source": [
    "total = pd.read_csv('./data/bookraw_total.csv',index_col=0)\n",
    "\n",
    "for row in total.iloc[:,1:].iterrows() :\n",
    "    row = row[1]\n",
    "\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def changeStringToList(strList):\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe안에 list를 통으로 넣으면 str으로 저장된다.\n",
    "    ast 라이브러리를 쓰면 원래 ㅣist 이지만 str 타입으로 표현된 값을 다시 list 타입으로 바꿔준다.\n",
    "\n",
    "    \"\"\"\n",
    "    return ast.literal_eval(strList)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도서 하나에 대한 모든 문장을 넣는 매소드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '데이터', '사이언티스트', '##를', '희망한다', '##면', '[MASK]', '##부터', '살펴', '##라', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '관련', '전공자', '##와', '석', '박사', '씻', '우대', '##하는', '현실', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] 79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def merge_row_items(row:pd.Series) : \n",
    "    '''\n",
    "    하나의 row에 있는 column을 통합하는 매소드\n",
    "    '''\n",
    "    str_list = row.tolist()\n",
    "\n",
    "    #'['asd']' => ['asd']\n",
    "    section_list = list(map(changeStringToList,str_list))\n",
    "\n",
    "    b = []\n",
    "    for a in section_list :\n",
    "        b += a\n",
    "\n",
    "    b = list(filter(None,b))\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "row_test = merge_row_items(row)[10:12]\n",
    "max_length = 40\n",
    "for i in range(0,len(row_test)-1) : \n",
    "    # 오리지널 버전 \n",
    "    first = tokenizer.encode(row_test[i])\n",
    "    \n",
    "    second = tokenizer.encode(row_test[i+1])\n",
    "\n",
    "    # mask, 단어 변경된 버전 \n",
    "    first = tokenizing_sentence(row_test[i],max_length=40)\n",
    "    second = tokenizing_sentence(row_test[i+1],max_length=40)\n",
    "    # 첫번째, 두번째 문장 합치기\n",
    "    sen,ids,mask = [first[i]+second[i][1:] for i in range(3)]\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현상황 정리\n",
    "\n",
    "배경\n",
    "1. NSP를 먼저 구성한 다음 MLM을 하는 식으로 구성해야함.\n",
    "2. NSP 구성 시 원본버전과 mask 버전 두개를 만들어야하는데 mask 버전을 먼저 만들다보니 복잡하게 구성됨\n",
    "\n",
    "할 일\n",
    "\n",
    "3. 원본 버전과 마스크 버전 매서드 개선하기\n",
    "4. class로 새롭게 구현하기."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
