10.27.(목)

### Fine tuning을 위해 Pretrained model과 해당 모델 학습에 활용된 pretrained tokenizer가 필요한 이유

**같은 단어라도 vocab의 id가 다르기 떄문**
A 모델이 노트북을 id 200으로 학습했는데 노트북 id가 1979로 된 새로운 토크나이저를 쓴다면 원하는 값을 얻지 못한다.

**OOV 방식이 개별 모델마다 다르기 때문(재검토 필요)**
OOV(Out of Vocabulary)

### Special Token 설명

**[SEP]**
Bert는 두 개의 문장을 하나로 합쳐 학습에 활용한다. 두 문장을 하나로 합친 것을 sequence라고 부른다. [SEP] 토큰은 sequence 내부에 두 문장을 구분하기 위해 사용한다. 한 번 학습에 두 문장을 사용하는 이유로는 Q&A 유형 학습 및 문장과 문장이 관련 있는지 학습하기 위함이다.

**[CLS]**
모든 sequence는 [CLS]로 시작한다. 학습 마지막 layer에서 산출된 Output의 [CLS]는 문장을 분류하는 용도로 활용된다고 한다.

> “The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.” (from the BERT paper)

**[PAD]**
[PAD]는 모든 sequence의 embedding dimension을 하나로 통일시키기 위해 사용된다. 어떤 VOCAB이라도 [PAD]에는 ID 0을 할당한다. 이는 [PAD]가 아닌 모든 Token을 1로 주는 새로운 리스트를 만든 다음 학습에는 [PAD]가 없는, 그러니까 1인 Token만 추려 학습한다.

> > 임베딩의 최대 문장 개수(maximum length)가 늘어나면 그만큼 학습에 투입되는 리소스가 늘어난다.

### Fine tuning OOV 문제는 어떻게 해결하지??

### 목표

bookraw 가지고 scratch 연습 및 도도모아 개선 해보자

1. W2V을 Bert로 대체하기 -> 학습한 Bert에 cosine sim을 찾으면 되지 않을까

2. Scratch 모델을 keyBert에 넣으면 더 효과적으로 추출이 가능하지 않을까

3. keyBert 원리는 어떻게 되는거지??

### 세부단계

1. 학습 자료 만들기

- Tokenizer 구현 : WordPiece로 구현 했음.
- Mask와 NSP 훈련용 데이터 생성

2. Distill Bert Model From Scratch(안보고 직접 구현해보기)

3. Cosine 유사도 활용하기
