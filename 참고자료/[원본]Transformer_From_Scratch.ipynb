{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class selfAttention(nn.Module) :\n",
    "    def __init__(self,embed_size, heads) -> None:\n",
    "        '''\n",
    "        embed_size : input 토큰 개수, 논문에서는 512개로 사용 \n",
    "        heads : multi_head의 개수, 논문에서는 8개 사용\n",
    "\n",
    "        Self Attention은 특정 단어(query)와 다른 단어(keys) 간의 중요도를 파악하는 매커니즘이다.\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embed_size = embed_size # 512차원\n",
    "        self.heads = heads # 8개\n",
    "        self.head_dim = embed_size // heads # 64차원(개별 attention의 차원)\n",
    "        \n",
    "\n",
    "        '''\n",
    "        dict에서 쓰는 key,value 와 같다.\n",
    "        query는 현재 찾고자 하는 값이다.\n",
    "        '''\n",
    "        # input feature, output feature\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False) \n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "        # Multi-headed attention을 만듬\n",
    "        # fully connected out \n",
    "        # input feature = outfut feature\n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size) # 64 * 8 \n",
    "        \n",
    "    def forward(self, values,keys,query,mask) :\n",
    "        N = query.shape[0] # 단어 개수\n",
    "        value_len = values.shape[1] # head 차원\n",
    "        key_len = keys.shape[1] \n",
    "        query_len = query.shape[1]\n",
    "\n",
    "        values = values.reshape(N,value_len, self.heads,self.head_dim)\n",
    "        keys = keys.reshape(N,key_len, self.heads,self.head_dim)\n",
    "        queries = query.reshape(N,query_len, self.heads,self.head_dim)\n",
    "\n",
    "        vlaues = self.values(values) \n",
    "        keys = self.values(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # score = Q dot K^T \n",
    "        score = torch.einsum(\"nqhd,nkhd->nhqk\", [queries,keys]) \n",
    "        # queries shape : N,value_len, self.heads,self.head_dim\n",
    "        # keys shape : N,key_len, self.heads,self.head_dim\n",
    "        # score shape : N, heads, query_len, key_len\n",
    "        \n",
    "        # decoder 구조인 masked Self Attention 적용 시 활용되는 구문\n",
    "        # score = -inf로 둬서 값을 예측 하도록 한다.\n",
    "        if mask is not None :\n",
    "            score = score.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            '''\n",
    "            mask = 0 인 값에 대해서 -inf 대입\n",
    "            -1e20 = -inf\n",
    "            -inf이기 때문에 값이 0에 수렴\n",
    "            mask가 부여된 경우 score 값을 0으로 준다.\n",
    "\n",
    "            '''\n",
    "        # attention 정의\n",
    "        attention = torch.softmax(score / (self.embed_size**(1/2)),dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd -> nqhd\",[attention, values]).reshape(\n",
    "            N,query_len,self.heads * self.head_dim\n",
    "            )\n",
    "        # attention shape : N, heads,query_len,key_len\n",
    "        # values shape : N, value_len, heads, heads_dim\n",
    "        # out shape : N, query_len, heads * head_dim\n",
    "\n",
    "        # concat all heads \n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module) :\n",
    "    def __init__(self,embed_size, heads, dropout, forward_expansion) -> None:\n",
    "        '''\n",
    "        embed_size : token 개수 | 논문 512개\n",
    "        heads : attention 개수 | 논문 8개\n",
    "        dropout : 골고루 학습하기 위한 방법론 \n",
    "        forward_expansion : forward 계산시 차원을 얼마나 늘릴 것인지 결정, 임의로 결정하는 값\n",
    "                            forward_차원 계산은 forward_expension * embed_size \n",
    "                            논문에서는 4로 정함. 총 2048차원으로 늘어남.\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        # Attention 정의\n",
    "        self.attention = selfAttention(embed_size,heads)\n",
    "        \n",
    "        ### Norm & Feed Forward\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forawrd = nn.Sequential(\n",
    "            # 차원을 512 -> 2048로 증가\n",
    "            nn.Linear(embed_size,forward_expansion*embed_size),\n",
    "            # 차원을 Relu 연산\n",
    "            nn.ReLU(),\n",
    "            # 차원 2048 -> 512로 축소 \n",
    "            nn.Linear(forward_expansion*embed_size,embed_size)\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    ### Encoder 구현 \n",
    "    def forward(self, value,key,query,mask) :\n",
    "        # self Attention\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        # Add & Normalization\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "        # Feed_Forward\n",
    "        forward = self.feed_forawrd(x)\n",
    "        # Add & Normalization\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "        ) -> None:\n",
    "        \n",
    "        '''\n",
    "        src_vocab_size : input vocab 개수  \n",
    "        num_layers : Encoder block 구현할 개수\n",
    "        dropout : dropout 비율\n",
    "        max_length : \n",
    "        '''\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "\n",
    "        # 시작부분 구현(input + positional_embeding)\n",
    "        self.word_embeding = nn.Embedding(src_vocab_size, embed_size) # row / col\n",
    "        self.position_embeding = nn.Embedding(max_length,embed_size) # row / col\n",
    "\n",
    "        # Transformer Layer 구현 \n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion = forward_expansion,\n",
    "                )\n",
    "            for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # dropout = 0 ~ 1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N,seq_length = x.shape # (a,b)\n",
    "        positions = torch.arange(0, seq_length).expand(N,seq_length).to(self.device) # (a,b)\n",
    "\n",
    "        out = self.dropout(self.word_embeding(x) + self.position_embeding(positions))\n",
    "\n",
    "        for layer in self.layers :\n",
    "            # query, key, value\n",
    "            out = layer(out,out,out,mask)\n",
    "            \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module) :\n",
    "    def __init__(self,embed_size, heads, forward_expansion, dropout, device) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = selfAttention(embed_size, heads=heads)\n",
    "        self.transfromer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,value,key,src_mask, target_mask) :\n",
    "        # output에 대한 attention 수행\n",
    "        attention = self.attention(x,x,x,target_mask)\n",
    "\n",
    "        # add & Norm\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        \n",
    "        # encoder_decoder attention + feed_forward \n",
    "        print(value.size(),key.size(),query.size())\n",
    "        out = self.transfromer_block(value, key, query, src_mask)\n",
    "        return out \n",
    "\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size, \n",
    "        embed_size, \n",
    "        num_layers, \n",
    "        heads, \n",
    "        forward_expansion, \n",
    "        dropout, \n",
    "        device, \n",
    "        max_length\n",
    "        \n",
    "        ) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length,embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderBlock(embed_size,heads,forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size,trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,enc_out, src_mask, trg_mask) :\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0,seq_length).expand(N,seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers :\n",
    "            x = layer(x,enc_out, enc_out, src_mask, trg_mask)\n",
    "        \n",
    "        out = self.fc_out(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module) :\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size = 512,\n",
    "        num_layers = 6,\n",
    "        forward_expansion = 4,\n",
    "        heads = 8,\n",
    "        dropout = 0,\n",
    "        device = 'cpu',\n",
    "        max_length = 100\n",
    "    ) -> None:\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.Encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(trg_vocab_size, \n",
    "            embed_size, \n",
    "            num_layers, \n",
    "            heads, \n",
    "            forward_expansion, \n",
    "            dropout, \n",
    "            device, \n",
    "            max_length)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def mask_src_mask(self,src) :\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #(N,1,1,src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def mask_trg_mask(self,trg) : \n",
    "        # trg = triangle\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N,1,trg_len,trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "    \n",
    "    def forward(self,src,trg) :\n",
    "        src_mask = self.mask_src_mask(src)\n",
    "        trg_mask = self.mask_trg_mask(trg)\n",
    "        enc_src = self.Encoder(src,src_mask)\n",
    "        out = self.decoder(trg,enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "### 작동 테스트\n",
    "device = 'cpu'\n",
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(\n",
    "    device\n",
    ")\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 100000\n",
    "trg_vocab_size = 10\n",
    "model = transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(\n",
    "    device\n",
    ")\n",
    "out = model(x, trg[:, :-1])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/[원본]Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/%5B%EC%9B%90%EB%B3%B8%5DTransformer_From_Scratch.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m a,b \u001b[39min\u001b[39;00m j[:\u001b[39m1\u001b[39m] :\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/%5B%EC%9B%90%EB%B3%B8%5DTransformer_From_Scratch.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/%5B%EC%9B%90%EB%B3%B8%5DTransformer_From_Scratch.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m model(a,b)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git_repo/transformer_from_scratch/model.py:340\u001b[0m, in \u001b[0;36mtransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    338\u001b[0m src_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[1;32m    339\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_trg_mask(trg)\n\u001b[0;32m--> 340\u001b[0m enc_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncoder(src, src_mask)\n\u001b[1;32m    341\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDecoder(trg, enc_src, src_mask, trg_mask)\n\u001b[1;32m    342\u001b[0m \u001b[39m# Linear Layer\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/git_repo/transformer_from_scratch/model.py:189\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m _, seq_len \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[1;32m    188\u001b[0m pos_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed[:, :seq_len, :]\n\u001b[0;32m--> 189\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeding(x) \u001b[39m+\u001b[39;49m pos_embed)\n\u001b[1;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    191\u001b[0m     \u001b[39m# Q,K,V,mask\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     out \u001b[39m=\u001b[39m layer(out, out, out, mask)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable, List\n",
    "from model import transformer\n",
    "from data import fr_to_en\n",
    "import utils_for_training as ut\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# param\n",
    "SRC_LANGUAGE = \"fr\"\n",
    "TGT_LANGUAGE = \"en\"\n",
    "\n",
    "def collate_fn(batch_iter: Iterable):\n",
    "    \"\"\"\n",
    "    Data_Loader에서 사용하는 매서드\n",
    "    \"\"\"\n",
    "    text_transform = {}\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        text_transform[ln] = ut.sequential_transforms(\n",
    "            token_transfrom[ln],  # 토큰화(Tokenization)\n",
    "            vocab_transform[ln],  # 수치화(Numericalization)\n",
    "            ut.tensor_transform,\n",
    "        )  # BOS/EOS를 추가하고 텐서를 생성\n",
    "    \n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch_iter:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n",
    "\n",
    "    PAD_IDX = 1\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "\n",
    "### Data_load\n",
    "fr_train = ut.open_text_set(\"data/training/train.fr\")\n",
    "en_train = ut.open_text_set(\"data/training/train.en\")\n",
    "vocab_transform, token_transfrom = ut.make_vocab(fr_train, en_train)\n",
    "\n",
    "\n",
    "# Load_Dataset\n",
    "dataset= fr_to_en(set_type='training')\n",
    "\n",
    "# Data_loader\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "# j = [i for i in train_dataloader]\n",
    "\n",
    "for a,b in j[:1] :\n",
    "    ''\n",
    "model(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
