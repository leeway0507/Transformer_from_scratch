{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "\n",
    "class BertDataSet(Dataset) :\n",
    "    def __init__(self,dir,index_col=0) -> None:\n",
    "        super().__init__()\n",
    "        self.data_load = pd.read_csv(dir,index_col=index_col)\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "        self.sentences = self._merge_sentences()\n",
    "\n",
    "    def _change_string_to_list(self,str_list):\n",
    "        \"\"\"\n",
    "\n",
    "        dataframe안에 list를 통으로 넣으면 str으로 저장된다.\n",
    "        ast 라이브러리를 쓰면 원래 ㅣist 이지만 str 타입으로 표현된 값을 다시 list 타입으로 바꿔준다.\n",
    "\n",
    "        \"\"\"\n",
    "        return ast.literal_eval(str_list)\n",
    "\n",
    "    def _merge_sentences(self) :\n",
    "        \"\"\"\n",
    "        \n",
    "        하나의 list 안에 모든 sentence 넣기\n",
    "\n",
    "        \"\"\"\n",
    "        total = []\n",
    "        for row in self.data_load.iterrows() :\n",
    "            book_info = row[1]\n",
    "            t = []\n",
    "            for i in range(1,4) :\n",
    "                t += self._change_string_to_list(book_info.iloc[i])\n",
    "\n",
    "            total += t\n",
    "        total = list(filter(None,total))\n",
    "        \n",
    "        return total\n",
    "\n",
    "    \n",
    "\n",
    "    # def __getitem__() :\n",
    "    #     pass\n",
    "    # def __call__(self, ) :\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 하나에 문장 전체 넣기\n",
    "# counter로 단어 중복 제거하기\n",
    "# vocab으로 index와 단어 매치하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = BertDataSet('./data/bookraw_total.csv')\n",
    "pd.DataFrame(test.sentences)[0].to_csv('./data/bookraw_list.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bookraw_list Text 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=673268, list='이 도서와 함께 지금 당장 이 시대의 가장 섹시한 직업 데이터 사이언티스트가 되어보세요')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_list = pd.read_csv('./data/bookraw_list.txt')\n",
    "\n",
    "preprocessed_list = []\n",
    "for i in text_list.itertuples() :\n",
    "    if pd.isna(i[1]) == False  :\n",
    "        if len(i[1]) > 3 :\n",
    "            val = i[1].lstrip('장 ')\n",
    "            val = val.lstrip(' ')\n",
    "            preprocessed_list.append(val)\n",
    "        \n",
    "pd.DataFrame(preprocessed_list).to_csv('./data/bookraw_list_v2.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece Tokenizer 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True, strip_accents=False)\n",
    "\n",
    "tokenizer.train('./data/bookraw_list.txt',vocab_size=100000,limit_alphabet=6000, min_frequency=10)\n",
    "\n",
    "# Tokenizing Test\n",
    "tokenizer.encode('python과 javascript를 만들며 고민했다').tokens\n",
    "\n",
    "# Saving Vocab\n",
    "tokenizer.save_model('.', 'bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing 한 것 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'python', '##과', 'javascript', '##를', '만들며', '고민', '##했다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "## 한글을 불러올 땐 strip accents = False 필수\n",
    "\n",
    "vocab = './data/vocab.txt'\n",
    "tokenizer = BertWordPieceTokenizer.from_file(vocab=vocab,strip_accents=False)\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode('python과 javascript를 만들며 고민했다')\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer로 daistillkobert와 kobert 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel,BertModel, AutoTokenizer\n",
    "\n",
    "model = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/distilkobert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzier max 개수 세기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 토큰 개수 30.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/distilkobert\")\n",
    "\n",
    "sentences = pd.read_csv('./data/bookraw_list_v2.txt')\n",
    "sentence_list = sentences.iloc[:,0]\n",
    "\n",
    "# nan 제거\n",
    "sentence_list = sentence_list[sentence_list.isna() == False].values\n",
    "\n",
    "# to numpy\n",
    "\n",
    "### token 상위 99% 이상인 문장 제거 \n",
    "num_sen = [len(sen.split(' ')) for sen in sentence_list.tolist()]\n",
    "\n",
    "percentile_99 = np.quantile(num_sen,0.99)\n",
    "\n",
    "sentence_list_under_99 = sentence_list[np.array(num_sen) < percentile_99 ].tolist()\n",
    "\n",
    "print(f\"최대 토큰 개수 {percentile_99}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다. [SEP]\")\n",
    "tokenizer.tokenize(\"[CLS] 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "# Base Model (108M)\n",
    "\n",
    "kc_tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"beomi/kcbert-base\")\n",
    "\n",
    "from kobert_tokenizer import KoBertTokenizer\n",
    "\n",
    "distill_tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistillBert Tokenizing 과 kcBert Tokenizing 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcBert tokenizer\n",
      "['[CLS]', '사이', '##몬', '##페', '##그', '##의', '익', '##살', '##스런', '연기', '##가', '돋', '##보', '##였던', '영화', '!', '스파이', '##더', '##맨', '##에서', '늙어', '##보이', '##기만', '했던', '커', '##스', '##틴', '던', '##스트', '##가', '너무나도', '이뻐', '##보', '##였다', '.', '[SEP]']\n",
      "\n",
      "['[CLS]', '막', '걸', '##음', '##마', '뗀', '3', '##세', '##부터', '초등학교', '1', '##학년', '##생', '##인', '8', '##살', '##용', '##영화', '.', 'ㅋㅋㅋ', '.', '.', '.', '별반', '##개도', '아까', '##움', '.', '[SEP]']\n",
      "\n",
      "distillbert tokenizer\n",
      "['[CLS]', '▁사이', '몬', '페', '그', '의', '▁익', '살', '스', '런', '▁연기', '가', '▁', '돋', '보', '였던', '▁영화', '!', '스', '파', '이', '더', '맨', '에서', '▁', '늙', '어', '보이', '기', '만', '▁했던', '▁커', '스', '틴', '▁', '던', '스트', '가', '▁너무', '나', '도', '▁이', '뻐', '보', '였다', '.', '[SEP]']\n",
      "\n",
      "['[CLS]', '▁막', '▁', '걸음', '마', '▁', '뗀', '▁3', '세', '부터', '▁초등학교', '▁1', '학년', '생', '인', '▁8', '살', '용', '영화', '.', 'ᄏ', 'ᄏ', 'ᄏ', '...', '별', '반', '개', '도', '▁아', '까', '움', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print('kcBert tokenizer')\n",
    "print(kc_tokenizer.tokenize(\"[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다. [SEP]\"))\n",
    "print('')\n",
    "print(kc_tokenizer.tokenize(\"[CLS] 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]\"))\n",
    "print('')\n",
    "print('distillbert tokenizer')\n",
    "print(distill_tokenizer.tokenize(\"[CLS] 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다. [SEP]\"))\n",
    "print('')\n",
    "print(distill_tokenizer.tokenize(\"[CLS] 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움. [SEP]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_max = 0\n",
    "max_num = 0\n",
    "for num, sen in enumerate(sentence_list_under_99) :\n",
    "    \n",
    "    len_sen = len(tokenizer.encode(sen))\n",
    "\n",
    "    if len_sen > len_max :\n",
    "        max_num = num\n",
    "\n",
    "    len_max = max(len_max,len_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_minimum: 31\n"
     ]
    }
   ],
   "source": [
    "print(f'max_length_minimum: {len_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning 용 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tokenizer.encode_plus function combines multiple steps for us:\n",
    "\n",
    "- Split the sentence into tokens.\n",
    "- Add the special [CLS] and [SEP] tokens.\n",
    "- Map the tokens to their IDs.m\n",
    "- Pad or truncate all sentences to the same length.\n",
    "- Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentence_list_under_99 : \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 40,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'메타버스 플랫폼의 차이점을 알아보자'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list_under_99[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [UNK] [UNK] [UNK] [SEP]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2,0,0,0,0,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
