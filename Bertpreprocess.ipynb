{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "\n",
    "class BertDataSet(Dataset) :\n",
    "    def __init__(self,dir,index_col=0) -> None:\n",
    "        super().__init__()\n",
    "        self.data_load = pd.read_csv(dir,index_col=index_col)\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "        self.sentences = self._merge_sentences()\n",
    "\n",
    "    def _change_string_to_list(self,str_list):\n",
    "        \"\"\"\n",
    "\n",
    "        dataframe안에 list를 통으로 넣으면 str으로 저장된다.\n",
    "        ast 라이브러리를 쓰면 원래 ㅣist 이지만 str 타입으로 표현된 값을 다시 list 타입으로 바꿔준다.\n",
    "\n",
    "        \"\"\"\n",
    "        return ast.literal_eval(str_list)\n",
    "\n",
    "    def _merge_sentences(self) :\n",
    "        \"\"\"\n",
    "        \n",
    "        하나의 list 안에 모든 sentence 넣기\n",
    "\n",
    "        \"\"\"\n",
    "        total = []\n",
    "        for row in self.data_load.iterrows() :\n",
    "            book_info = row[1]\n",
    "            t = []\n",
    "            for i in range(1,4) :\n",
    "                t += self._change_string_to_list(book_info.iloc[i])\n",
    "\n",
    "            total += t\n",
    "        total = list(filter(None,total))\n",
    "        \n",
    "        return total\n",
    "\n",
    "    \n",
    "\n",
    "    # def __getitem__() :\n",
    "    #     pass\n",
    "    # def __call__(self, ) :\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 하나에 문장 전체 넣기\n",
    "# counter로 단어 중복 제거하기\n",
    "# vocab으로 index와 단어 매치하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = BertDataSet('./data/bookraw_total.csv')\n",
    "pd.DataFrame(test.sentences)[0].to_csv('./data/bookraw_list.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python과', 'javascript를', '만들며', '고민', '##했다']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True, strip_accents=False)\n",
    "\n",
    "tokenizer.train('./data/bookraw_list.txt',vocab_size=100000,limit_alphabet=6000, min_frequency=10)\n",
    "\n",
    "# Tokenizing Test\n",
    "tokenizer.encode('python과 javascript를 만들며 고민했다').tokens\n",
    "\n",
    "# Saving Vocab\n",
    "tokenizer.save_model('.', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'python', '##과', 'javascript', '##를', '만들며', '고민', '##했다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## Tokenizer 불러오기\n",
    "## 한글을 불러올 땐 strip accents = False 필수\n",
    "\n",
    "vocab = './data/vocab.txt'\n",
    "tokenizer = BertWordPieceTokenizer.from_file(vocab=vocab,strip_accents=False)\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode('python과 javascript를 만들며 고민했다')\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel,BertModel, AutoTokenizer\n",
    "\n",
    "a = DistilBertModel.from_pretrained('monologg/distilkobert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/distilkobert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
