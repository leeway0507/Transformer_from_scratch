{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "\n",
    "class BertDataSet(Dataset) :\n",
    "    def __init__(self,dir,index_col=0) -> None:\n",
    "        super().__init__()\n",
    "        self.data_load = pd.read_csv(dir,index_col=index_col)\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "        self.sentences = self._merge_sentences()\n",
    "\n",
    "    def _change_string_to_list(self,str_list):\n",
    "        \"\"\"\n",
    "\n",
    "        dataframe안에 list를 통으로 넣으면 str으로 저장된다.\n",
    "        ast 라이브러리를 쓰면 원래 ㅣist 이지만 str 타입으로 표현된 값을 다시 list 타입으로 바꿔준다.\n",
    "\n",
    "        \"\"\"\n",
    "        return ast.literal_eval(str_list)\n",
    "\n",
    "    def _merge_sentences(self) :\n",
    "        \"\"\"\n",
    "        \n",
    "        하나의 list 안에 모든 sentence 넣기\n",
    "\n",
    "        \"\"\"\n",
    "        total = []\n",
    "        for row in self.data_load.iterrows() :\n",
    "            book_info = row[1]\n",
    "            t = []\n",
    "            for i in range(1,4) :\n",
    "                t += self._change_string_to_list(book_info.iloc[i])\n",
    "\n",
    "            total += t\n",
    "        total = list(filter(None,total))\n",
    "        \n",
    "        return total\n",
    "\n",
    "    \n",
    "\n",
    "    # def __getitem__() :\n",
    "    #     pass\n",
    "    # def __call__(self, ) :\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 하나에 문장 전체 넣기\n",
    "# counter로 단어 중복 제거하기\n",
    "# vocab으로 index와 단어 매치하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = BertDataSet('./data/bookraw_total.csv')\n",
    "pd.DataFrame(test.sentences)[0].to_csv('./data/bookraw_list.txt',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordpiece Tokenizer 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=True, strip_accents=False)\n",
    "\n",
    "tokenizer.train('./data/bookraw_list.txt',vocab_size=100000,limit_alphabet=6000, min_frequency=10)\n",
    "\n",
    "# Tokenizing Test\n",
    "tokenizer.encode('python과 javascript를 만들며 고민했다').tokens\n",
    "\n",
    "# Saving Vocab\n",
    "tokenizer.save_model('.', 'bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing 한 것 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'python', '##과', 'javascript', '##를', '만들며', '고민', '##했다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "## 한글을 불러올 땐 strip accents = False 필수\n",
    "\n",
    "vocab = './data/vocab.txt'\n",
    "tokenizer = BertWordPieceTokenizer.from_file(vocab=vocab,strip_accents=False)\n",
    "\n",
    "\n",
    "encoded = tokenizer.encode('python과 javascript를 만들며 고민했다')\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer로 daistillkobert와 kobert 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/distilkobert were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel,BertModel, AutoTokenizer\n",
    "\n",
    "model = DistilBertModel.from_pretrained('monologg/distilkobert')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/distilkobert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzier max 개수 세기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 토큰 개수 30.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/distilkobert\")\n",
    "\n",
    "sentences = pd.read_csv('./data/bookraw_list.txt')\n",
    "sentence_list = sentences.iloc[:,0]\n",
    "\n",
    "# nan 제거\n",
    "sentence_list = sentence_list[sentence_list.isna() == False].values\n",
    "\n",
    "# to numpy\n",
    "\n",
    "### token 상위 99% 이상인 문장 제거 \n",
    "num_sen = [len(sen.split(' ')) for sen in sentence_list.tolist()]\n",
    "\n",
    "percentile_99 = np.quantile(num_sen,0.99)\n",
    "\n",
    "sentence_list_under_99 = sentence_list[np.array(num_sen) < percentile_99 ].tolist()\n",
    "\n",
    "print(f\"최대 토큰 개수 {percentile_99}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_max = 0\n",
    "max_num = 0\n",
    "for num, sen in enumerate(sentence_list_under_99) :\n",
    "    \n",
    "    len_sen = len(tokenizer.encode(sen))\n",
    "\n",
    "    if len_sen > len_max :\n",
    "        max_num = num\n",
    "\n",
    "    len_max = max(len_max,len_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_minimum: 31\n"
     ]
    }
   ],
   "source": [
    "print(f'max_length_minimum: {len_max}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning 용 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tokenizer.encode_plus function combines multiple steps for us:\n",
    "\n",
    "- Split the sentence into tokens.\n",
    "- Add the special [CLS] and [SEP] tokens.\n",
    "- Map the tokens to their IDs.\n",
    "- Pad or truncate all sentences to the same length.\n",
    "- Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentence_list_under_99 : \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 40,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'메타버스 플랫폼의 차이점을 알아보자'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list_under_99[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [UNK] [UNK] [UNK] [UNK] [SEP]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2,0,0,0,0,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
