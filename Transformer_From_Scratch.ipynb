{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenzier 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable, List\n",
    "from model import transformer\n",
    "from data import fr_to_en\n",
    "import utils_for_training as ut\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "### Data_load\n",
    "fr_train = ut.open_text_set(\"data/training/train.fr\")\n",
    "en_train = ut.open_text_set(\"data/training/train.en\")\n",
    "vocab_transform, token_transfrom = ut.make_vocab(fr_train, en_train)\n",
    "\n",
    "decoder_en = {v:k for k,v in vocab_transform['en'].get_stoi().items()}\n",
    "decoder_fr = {v:k for k,v in vocab_transform['fr'].get_stoi().items()}\n",
    "\n",
    "# param\n",
    "SRC_LANGUAGE = \"fr\"\n",
    "TGT_LANGUAGE = \"en\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Parameter\n",
      "{'src_vocab_size': 11509, 'trg_vocab_size': 10837, 'src_pad_idx': 1, 'trg_pad_idx': 1, 'embed_size': 512, 'num_layers': 3, 'forward_expansion': 2, 'heads': 8, 'dropout': 0.1, 'device': 'mps', 'max_length': 140}\n"
     ]
    }
   ],
   "source": [
    "with open('config/transformer.json', 'r') as file:\n",
    "    param = json.load(file)\n",
    "    print('Model_Parameter')\n",
    "    print(param)    \n",
    "model = transformer(**param)\n",
    "device = param['device']\n",
    "\n",
    "# xavier \n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# loss_fn\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1)\n",
    "\n",
    "# optimzer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_iter: Iterable):\n",
    "    \"\"\"\n",
    "    Data_Loader에서 사용하는 매서드\n",
    "    \"\"\"\n",
    "    text_transform = {}\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        text_transform[ln] = ut.sequential_transforms(\n",
    "            token_transfrom[ln],  # 토큰화(Tokenization)\n",
    "            vocab_transform[ln],  # 수치화(Numericalization)\n",
    "            ut.tensor_transform,\n",
    "        )  # BOS/EOS를 추가하고 텐서를 생성\n",
    "    \n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch_iter:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n",
    "\n",
    "    PAD_IDX = 1\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch.T, tgt_batch.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,optimizer) : \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    # Load_Dataset\n",
    "    dataset= fr_to_en(set_type='training')\n",
    "\n",
    "    # Data_loader\n",
    "    batch_size = 128\n",
    "    train_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "    # j = [i for i in train_dataloader]\n",
    "\n",
    "    for src,tgt in train_dataloader :\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        logits = model(src,tgt)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    # Load_Dataset\n",
    "    dataset= fr_to_en(set_type='validation')\n",
    "\n",
    "    # Data_loader\n",
    "    batch_size = 128\n",
    "    val_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "    for i,(src,tgt) in enumerate(val_dataloader) :\n",
    "        # tgt_input = tgt[:-1, :]\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        logits = model(src,tgt)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads) -> None:\n",
    "        \"\"\"\n",
    "        embed_size : input 토큰 개수, 논문에서는 512개로 사용\n",
    "        heads : multi_head의 개수, 논문에서는 8개 사용\n",
    "        Self Attention은 특정 단어(query)와 다른 단어(key) 간의 중요도를 파악하는 매커니즘이다.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size  # 512차원\n",
    "        self.heads = heads  # 8개\n",
    "        self.head_dim = embed_size // heads  # 64차원(개별 attention의 차원)\n",
    "        \"\"\"\n",
    "        query는 기준이 되는 token 모음\n",
    "        key는 문장 내 token 모음\n",
    "        모든 token이 query로 활용되므로\n",
    "        모든 token의 개별 token 간 연관성 파악이 가능\n",
    "        \"\"\"\n",
    "        # input feature, output feature\n",
    "        self.value = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64\n",
    "        self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64\n",
    "        self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)  # 64 => 64\n",
    "        # Multi-headed attention을 만듬\n",
    "        # fully connected out\n",
    "        # input feature = outfut feature\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)  # 64 * 8 => 512\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        \"\"\"\n",
    "        # query, key, value: (문장 개수(n) x 최대 token 개수(=100) x embeding 차원(=512) )\n",
    "        \"\"\"\n",
    "\n",
    "        N_batch = query.shape[0]  # 총 문장 개수\n",
    "        value_len = value.shape[1]  # token 개수\n",
    "        key_len = key.shape[1]  # token 개수\n",
    "        query_len = query.shape[1]  # token 개수\n",
    "\n",
    "        value = value.reshape(\n",
    "            N_batch, self.heads, value_len, self.head_dim\n",
    "        )  # (n x h x value_len x d_k)\n",
    "        key = key.reshape(\n",
    "            N_batch, self.heads, key_len, self.head_dim\n",
    "        )  # (n x h x key_len x d_k)\n",
    "        query = query.reshape(\n",
    "            N_batch, self.heads, query_len, self.head_dim\n",
    "        )  # (n x h x query_len x d_k)\n",
    "\n",
    "        # Q,K,V 구하기\n",
    "        V = self.value(value)\n",
    "        K = self.key(key)\n",
    "        Q = self.query(query)\n",
    "\n",
    "        # score = Q dot K^T\n",
    "        # score = torch.einsum(\"nqhd,nkhd->nhqk\", [query,key])\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        # query shape : (n x h x query_len x d_k)\n",
    "        # key shape : (n x h x d_k x key_len)\n",
    "        # score shape : (n x h x query_len x key_len)\n",
    "        # Pad 부분을 0 => -inf로 변환하는 과정\n",
    "        #\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "            \"\"\"\n",
    "            mask = 0 인 값에 대해서 -inf 대입\n",
    "            -1e20 = -inf\n",
    "            -inf이기 때문에 softmax 계산시 값 0을 부여받음\n",
    "            \"\"\"\n",
    "        # attention 정의\n",
    "        # parameter dim은 몇번째 값에 softmax를 수행하는지 설정함.\n",
    "        softmax_score = torch.softmax(score / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # out = torch.einsum(\"nhql,nlhd -> nqhd\",[attention, value]).reshape(\n",
    "        #     N,query_len,self.heads * self.head_dim\n",
    "        #     )\n",
    "        # out = torch.matmul(softmax_score,V).reshape(\n",
    "        #     N,query_len,self.heads * self.head_dim\n",
    "        #     )\n",
    "        out = torch.matmul(softmax_score, V).reshape(\n",
    "            N_batch, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        # softmax_score shape : (n x h x query_len x key_len)\n",
    "        # value shape : (n x h x value_len x d_k)\n",
    "        # (value_len과 key_len은 size가 같음.)\n",
    "        # out shape : (n x h x query_len x d_k)\n",
    "        # transpose shape : (n x query_len x embed_size)\n",
    "        # concat all heads\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion) -> None:\n",
    "        \"\"\"\n",
    "        embed_size : token 개수 | 논문 512개\n",
    "        heads : attention 개수 | 논문 8개\n",
    "        dropout : 개별 Node를 골고루 학습하기 위한 방법론\n",
    "        forward_expansion : forward 계산시 차원을 얼마나 늘릴 것인지 결정, 임의로 결정하는 값\n",
    "                            forward_차원 계산은 forward_expension * embed_size\n",
    "                            논문에서는 4로 정함. 총 2048차원으로 늘어남.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Attention 정의\n",
    "        self.attention = selfAttention(embed_size, heads)\n",
    "        ### Norm & Feed Forward\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forawrd = nn.Sequential(\n",
    "            # 차원을 512 -> 2048로 증가\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            # 차원을 ReLU 연산\n",
    "            nn.ReLU(),\n",
    "            # 차원 2048 -> 512로 축소\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    ### Encoder Block 구현\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # self Attention\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "        # Add & Normalization\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        # Feed_Forward\n",
    "        forward = self.feed_forawrd(x)\n",
    "        # Add & Normalization\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        src_vocab_size : input vocab 개수\n",
    "        num_layers : Encoder block 구현할 개수\n",
    "        dropout : dropout 비율 0 ~ 1사이\n",
    "        max_length : 문장 내 최대 token 개수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "\n",
    "        # 시작부분 구현(input + positional_embeding)\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)  # row / col\n",
    "\n",
    "        # positional embedding\n",
    "        pos_embed = torch.zeros(max_length, embed_size)\n",
    "        pos_embed.requires_grad = False\n",
    "        position = torch.arange(0, max_length).float().unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size)\n",
    "        )\n",
    "        pos_embed[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_embed = pos_embed.unsqueeze(0).to(device)\n",
    "\n",
    "        # Encoder Layer 구현\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # dropout = 0 ~ 1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        _, seq_len = x.size()\n",
    "\n",
    "        pos_embed = self.pos_embed[:, :seq_len, :]  # 2 -> 3차원으로 늘림\n",
    "        out = self.dropout(self.word_embedding(x) + pos_embed)\n",
    "        for layer in self.layers:\n",
    "            # Q,K,V,mask\n",
    "            out = layer(out, out, out, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device) -> None:\n",
    "        \"\"\"\n",
    "        embed_size : token 개수 | 논문 512개\n",
    "        heads : attention 개수 | 논문 8개\n",
    "        dropout : 개별 Node를 골고루 학습하기 위한 방법론\n",
    "        forward_expansion : forward 계산시 차원을 얼마나 늘릴 것인지 결정, 임의로 결정하는 값\n",
    "                            forward_차원 계산은 forward_expension * embed_size\n",
    "                            논문에서는 4로 정함. 총 2048차원으로 늘어남.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = selfAttention(embed_size, heads=heads)\n",
    "        self.encoder_block = EncoderBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_trg_mask, target_mask):\n",
    "        # x = decoder_input\n",
    "        # value,key = encoder_input\n",
    "        # src_trg_mask : Multi-head attention에서 Pad에 대한 Mask 수행\n",
    "        # target_mask : Masked Multi-head attention에서 Teacher Forcing을 위한 Mask 수행\n",
    "        # output에 대한 attention 수행\n",
    "\n",
    "        # masked_attention\n",
    "        attention = self.attention(x, x, x, target_mask)\n",
    "        # add & Norm\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "\n",
    "        # encoder_decoder attention + feed_forward\n",
    "        out = self.encoder_block(value, key, query, src_trg_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        trg_vocab_size : input vocab 개수\n",
    "        embed_size : embedding_size\n",
    "        num_layers : Encoder block 구현할 개수\n",
    "        dropout : dropout 비율 0 ~ 1사이\n",
    "        max_length : 문장 내 최대 token 개수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 시작부분 구현(input + positional_embeding)\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "\n",
    "        # positional embedding\n",
    "        pos_embed = torch.zeros(max_length, embed_size)\n",
    "        pos_embed.requires_grad = False\n",
    "        position = torch.arange(0, max_length).float().unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_size, 2) * -(math.log(10000.0) / embed_size)\n",
    "        )\n",
    "        pos_embed[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_embed[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_embed = pos_embed.unsqueeze(0).to(device)\n",
    "\n",
    "        # Decoder Layer 구현\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_trg_mask, trg_mask):\n",
    "        # N, seq_length = x.shape\n",
    "        # positional embedding\n",
    "        _, seq_len = x.size()\n",
    "        pos_embed = self.pos_embed[:, :seq_len, :]\n",
    "        out = self.dropout(self.word_embedding(x) + pos_embed)\n",
    "        for layer in self.layers:\n",
    "            # Decoder Input, Encoder K, Encoder V , src_trg_mask, trg_mask\n",
    "            out = layer(out, enc_out, enc_out, src_trg_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ntransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        device=\"cpu\",\n",
    "        max_length=100,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.Encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "        self.Decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "        # Probability Generlator\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "\n",
    "    def encode(self, src):\n",
    "        src_mask = self.make_pad_mask(src,src)\n",
    "        return self.Encoder(src, src_mask)\n",
    "\n",
    "    def decode(self,src,trg,enc_src):\n",
    "        # decode\n",
    "        src_trg_mask = self.make_pad_mask(trg, src)  # Decoder Input에 대한 masking 필요\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        out = self.Decoder(trg, enc_src, src_trg_mask, trg_mask)\n",
    "        # Linear Layer\n",
    "        out = self.fc_out(out)  # num of sentence x max_length x trg_vocab_size\n",
    "\n",
    "        # Softmax\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n",
    "\n",
    "    def make_pad_mask(self, query, key):\n",
    "        len_query, len_key = query.size(1), key.size(1)\n",
    "        \n",
    "        # batch_size x 1 x 1 x len_key\n",
    "        key = key.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_query x len_key\n",
    "        key = key.repeat(1, 1, len_query, 1)\n",
    "\n",
    "        # batch_size x 1 x len_query x 1\n",
    "        query = query.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_query x len_key\n",
    "        query = query.repeat(1, 1, 1, len_key)\n",
    "\n",
    "        mask = key & query\n",
    "        return mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg = triangle\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        src_trg_mask = self.make_pad_mask(trg, src)  # Decoder Input에 대한 masking 필요\n",
    "        enc_src = self.Encoder(src, src_mask)\n",
    "        out = self.Decoder(trg, enc_src, src_trg_mask, trg_mask)\n",
    "        # Linear Layer\n",
    "        out = self.fc_out(out)  # num of sentence x max_length x trg_vocab_size\n",
    "\n",
    "        # Softmax\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Parameter\n",
      "{'src_vocab_size': 11509, 'trg_vocab_size': 10837, 'src_pad_idx': 1, 'trg_pad_idx': 1, 'embed_size': 512, 'num_layers': 3, 'forward_expansion': 2, 'heads': 8, 'dropout': 0.1, 'device': 'mps', 'max_length': 140}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rh/7xnmdybn7778kzq927hfcn8c0000gn/T/ipykernel_26960/3700883533.py:359: UserWarning: The operator 'aten::bitwise_and.Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  mask = key & query\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(model, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     end_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model)\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m src \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m tgt \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m logits \u001b[39m=\u001b[39m model(src,tgt)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# tgt_out = tgt[1:,:]\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mntransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=371'>372</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_trg_mask(trg)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=372'>373</a>\u001b[0m src_trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_pad_mask(trg, src)  \u001b[39m# Decoder Input에 대한 masking 필요\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=373'>374</a>\u001b[0m enc_src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mEncoder(src, src_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=374'>375</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDecoder(trg, enc_src, src_trg_mask, trg_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=375'>376</a>\u001b[0m \u001b[39m# Linear Layer\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m _, seq_len \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m pos_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed[:, :seq_len, :]  \u001b[39m# 2 -> 3차원으로 늘림\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embedding(x) \u001b[39m+\u001b[39m pos_embed)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     \u001b[39m# Q,K,V,mask\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X36sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m     out \u001b[39m=\u001b[39m layer(out, out, out, mask)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "with open('config/transformer.json', 'r') as file:\n",
    "    param = json.load(file)\n",
    "    print('Model_Parameter')\n",
    "    print(param)    \n",
    "model = ntransformer(**param)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Parameter\n",
      "{'src_vocab_size': 11509, 'trg_vocab_size': 10837, 'src_pad_idx': 1, 'trg_pad_idx': 1, 'embed_size': 512, 'num_layers': 3, 'forward_expansion': 2, 'heads': 8, 'dropout': 0.1, 'device': 'cpu', 'max_length': 140}\n",
      "5 5\n",
      "1 5\n",
      "1 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 8, 5, 64]' is invalid for input of size 2560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_len\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   memory\u001b[39m=\u001b[39m b\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(src,ys,memory)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   _, next_word \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(out,dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m   next_word \u001b[39m=\u001b[39m next_word\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mntransformer.decode\u001b[0;34m(self, src, trg, enc_src)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m src_trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_pad_mask(trg, src)  \u001b[39m# Decoder Input에 대한 masking 필요\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_trg_mask(trg)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDecoder(trg, enc_src, src_trg_mask, trg_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m \u001b[39m# Linear Layer\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=339'>340</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)  \u001b[39m# num of sentence x max_length x trg_vocab_size\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, enc_out, src_trg_mask, trg_mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embedding(x) \u001b[39m+\u001b[39m pos_embed)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=280'>281</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=281'>282</a>\u001b[0m     \u001b[39m# Decoder Input, Encoder K, Encoder V , src_trg_mask, trg_mask\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=282'>283</a>\u001b[0m     out \u001b[39m=\u001b[39m layer(out, enc_out, enc_out, src_trg_mask, trg_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=283'>284</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, value, key, src_trg_mask, target_mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=222'>223</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(attention \u001b[39m+\u001b[39m x))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=224'>225</a>\u001b[0m \u001b[39m# encoder_decoder attention + feed_forward\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=225'>226</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_block(value, key, query, src_trg_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=226'>227</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, value, key, query, mask)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, value, key, query, mask):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     \u001b[39m# self Attention\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(value, key, query, mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     \u001b[39m# Add & Normalization\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(attention \u001b[39m+\u001b[39m query))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb 셀 7\u001b[0m in \u001b[0;36mselfAttention.forward\u001b[0;34m(self, value, key, query, mask)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m key_len \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]  \u001b[39m# token 개수\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m query_len \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]  \u001b[39m# token 개수\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     N_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheads, value_len, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m )  \u001b[39m# (n x h x value_len x d_k)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m key \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     N_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, key_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m )  \u001b[39m# (n x h x key_len x d_k)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m query \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     N_batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, query_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/transformer_from_scratch/Transformer_From_Scratch.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m )  \u001b[39m# (n x h x query_len x d_k)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 8, 5, 64]' is invalid for input of size 2560"
     ]
    }
   ],
   "source": [
    "src = torch.tensor([[2,2012, 4805, 1536,3]])\n",
    "ys = torch.ones(1, 1).fill_(2).type(torch.long)\n",
    "b = model.encode(src)\n",
    "max_len = 10\n",
    "for i in range(max_len-1):\n",
    "  memory= b\n",
    "  out = model.decode(src,ys,memory)\n",
    "  _, next_word = torch.max(out,dim=-1)\n",
    "  next_word = next_word.item()\n",
    "  ys = torch.cat([ys,\n",
    "                  torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "  if next_word == 3:\n",
    "      break\n",
    "print(ys)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param\n",
    "SRC_LANGUAGE = \"fr\"\n",
    "TGT_LANGUAGE = \"en\"\n",
    "\n",
    "with open('config/transformer.json', 'r') as file:\n",
    "    param = json.load(file)\n",
    "    print('Model_Parameter')\n",
    "    print(param)    \n",
    "model = ntransformer(**param)\n",
    "\n",
    "def train_epoch(model,optimizer) : \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    # Load_Dataset\n",
    "    dataset= fr_to_en(set_type='training')\n",
    "\n",
    "    # Data_loader\n",
    "    batch_size = 128\n",
    "    train_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "    j = [i for i in train_dataloader]\n",
    "\n",
    "    # for src,tgt in train_dataloader :\n",
    "    for src,tgt in train_dataloader :\n",
    "        \n",
    "        # tgt_input = tgt[:, :-1]\n",
    "        logits = model(src,tgt)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # tgt_out = tgt[:,1:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "        \n",
    "\n",
    "train_loss = train_epoch(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suis'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_to_en(result) :\n",
    "    encoding_val = result.argmax().item()\n",
    "    return decoder_en[encoding_val]\n",
    "\n",
    "def decode_to_fr(result) :\n",
    "    if type(result) == torch.Tensor() :\n",
    "        encoding_val = result.argmax().item()\n",
    "    elif type(result) == int :\n",
    "        encoding_val = result\n",
    "    return decoder_fr[encoding_val]\n",
    "\n",
    "\n",
    "decode_to_en(b)\n",
    "decode_to_fr(4805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 108]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform['fr'](['bleu','vert'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 진행\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.730595707893372"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    # Load_Dataset\n",
    "    dataset= fr_to_en(set_type='validation')\n",
    "\n",
    "    # Data_loader\n",
    "    batch_size = 128\n",
    "    val_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "    for i,(src,tgt) in enumerate(val_dataloader) :\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        \n",
    "        logits = model(src,tgt)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:,1:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        if i % 20 == 0 :\n",
    "            print(f'{i}번째 진행')\n",
    "    return losses / len(val_dataloader)\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    text_transform = {}\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        text_transform[ln] = ut.sequential_transforms(\n",
    "            token_transfrom[ln],  # 토큰화(Tokenization)\n",
    "            vocab_transform[ln],  # 수치화(Numericalization)\n",
    "            ut.tensor_transform,\n",
    "        )  # BOS/EOS를 추가하고 텐서를 생성\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    model(src)\n",
    "\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2012, 4805, 1536]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
