{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterable, List\n",
    "from model import Transformer\n",
    "from data import fr_to_en\n",
    "import utils\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 불러오기\n",
    "# Fr -> En 번역을 위한 데이터셋(Multi-30k) 활용\n",
    "fr_train = utils.open_text_set(\"data/training/train.fr\")\n",
    "en_train = utils.open_text_set(\"data/training/train.en\")\n",
    "\n",
    "# Vocab 만들기 / 관련 함수는 utils.py 참조\n",
    "try : \n",
    "  vocab_transform, token_transform = utils.make_vocab(fr_train, en_train)\n",
    "except :  \n",
    "  # 오류 발생 시 spacy 설치 필요\n",
    "\n",
    "  # spacy tokenizer 다운로드(en,fr)\n",
    "  import spacy.cli\n",
    "  spacy.cli.download(\"en_core_web_sm\")\n",
    "  spacy.cli.download(\"fr_core_news_sm\")\n",
    "  vocab_transform, token_transform = utils.make_vocab(fr_train, en_train)\n",
    "\n",
    "# param\n",
    "SRC_LANGUAGE = \"fr\"\n",
    "TGT_LANGUAGE = \"en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습한 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Parameters\n",
      "--------------------------------------------------\n",
      "{'src_vocab_size': 11509, 'trg_vocab_size': 10837, 'src_pad_idx': 1, 'trg_pad_idx': 1, 'embed_size': 512, 'num_layers': 3, 'forward_expansion': 2, 'heads': 8, 'dropout': 0.1, 'device': 'cpu', 'max_length': 140}\n",
      "--------------------------------------------------\n",
      "현재 devicde 설정값은 : \"cpu\" 입니다. 변경을 희망하실 경우 config/transformer.json을 수정해주세요.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('config/transformer.json', 'r') as file:\n",
    "    param = json.load(file)\n",
    "    print('Model_Parameters')\n",
    "    print('-'*50)\n",
    "    print(param)  \n",
    "\n",
    "# multi-30k 데이터를 20번 epoch한 모델\n",
    "model = Transformer(**param)\n",
    "\n",
    "# model 불러오기\n",
    "model.load_state_dict(torch.load('model/model.pth'))\n",
    "\n",
    "# 모델 평가모드로 변경\n",
    "model.eval()\n",
    "\n",
    "\n",
    "device = model.device\n",
    "\n",
    "print('-'*50)\n",
    "print(f'현재 devicde 설정값은 : \"{model.device}\" 입니다. 변경을 희망하실 경우 config/transformer.json을 수정해주세요.')\n",
    "print('-'*50)\n",
    "\n",
    "# loss_fn\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능 테스트\n",
    "\n",
    "* validation은 문제와 정답이 모두 주어진다면 test는 문제만 주는 상황임.\n",
    "\n",
    "* test 함수를 통해 Transformer의 실제 문제 예측 과정을 이해할 수 있음.\n",
    "\n",
    "* Transformer는 문제와 정답이 있다면 답을 구하는 과정을 병렬적으로 수행할 수 있음.\n",
    "\n",
    "* 하지만 테스트에서는 정답이 주어지지 않으므로 한 번의 하나의 토큰을 생산함.\n",
    "\n",
    "* < bos > token을 시작으로 다음 토큰을 예상하며 < eos > 토큰이 생성될때까지 반복적으로 예측을 수행하게 되는 알고리즘이 필요함.\n",
    "\n",
    "* 아래의 test 함수를 다뤄보면서 Transformer의 데이터 처리 과정을 이해할 수 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : Deux chiens courent l'un vers l'autre sur le sable.\n",
      "모델예측 : Two young dog running down a beach , one is walking on the beach\n",
      "정답 : Two dogs are running towards each other across the sand.\n",
      "\n",
      "주의! 29,000개의 제한된 데이터로 학습을 수행했으므로 완벽한 예측이 불가능함.\n"
     ]
    }
   ],
   "source": [
    "def tokenizing_src(input_data:str) : \n",
    "    # input_data_tokenizing\n",
    "    token_data = token_transform['fr'](input_data)\n",
    "    vocab_src = vocab_transform['fr'](token_data)\n",
    "    tokenized_src = [2] + vocab_src + [3]\n",
    "    return tokenized_src\n",
    "\n",
    "def select_random_item() :\n",
    "    num = torch.randint(1,29000,(1,)).item()\n",
    "\n",
    "    return fr_train[num], en_train[num]\n",
    "\n",
    "def test(model) :\n",
    "    '''\n",
    "    * validation은 문제와 정답이 모두 주어진다면 test는 문제만 주는 상황임.\n",
    "\n",
    "    * test 함수를 통해 Transformer의 실제 문제 예측 과정을 이해할 수 있음.\n",
    "\n",
    "    * Transformer는 문제와 정답이 있다면 답을 구하는 과정을 병렬적으로 수행할 수 있음.\n",
    "\n",
    "    * 하지만 테스트에서는 정답이 주어지지 않으므로 한 번의 하나의 토큰을 생산함.\n",
    "\n",
    "    * < bos > token을 시작으로 다음 토큰을 예상하며 < eos > 토큰이 생성될때까지 반복적으로 예측을 수행하게 되는 알고리즘이 필요함.\n",
    "\n",
    "    * 아래의 test 함수를 다뤄보면서 Transformer의 데이터 처리 과정을 이해할 수 있음.\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 임의의 훈련 데이터 선별\n",
    "    fr_item, en_item = select_random_item()\n",
    "    \n",
    "    print('입력 :', fr_item)\n",
    "\n",
    "    # Input Data 토크나이징 \n",
    "    tokenized_input = tokenizing_src(fr_item)\n",
    "    max_length = int(len(tokenized_input) * 1.2)\n",
    "\n",
    "    # src Tensor에 Token 저장\n",
    "    src = torch.LongTensor(tokenized_input).unsqueeze(0).to(device)\n",
    "\n",
    "    # trg Tensor 생성(1, max_length)\n",
    "    trg = torch.zeros(1,max_length).type_as(src.data).to(device)\n",
    "\n",
    "    # src encoding\n",
    "    enc_src = model.encode(src)\n",
    "\n",
    "    next_trg = 2 # 문장 시작 <bos> idx\n",
    "\n",
    "    # 문장 예측 시작\n",
    "    for i in range(0,max_length) :\n",
    "        trg[0][i] = next_trg # token 저장\n",
    "\n",
    "        logits = model.decode(src,trg,enc_src) # output 산출\n",
    "\n",
    "        prd = logits.squeeze(0).max(dim=-1, keepdim=False)[1] # 예측 단어 중 max 추출\n",
    "        next_word = prd.data[i] # i 번째 위치한 단어 추출\n",
    "        next_trg = next_word.item() \n",
    "        if next_trg == 3 :\n",
    "            # <eos> 나오면 종료\n",
    "            trg[0][i] = next_trg\n",
    "            break\n",
    "    \n",
    "    # <pad> 제거\n",
    "    if 3 in trg[0] :\n",
    "        eos_idx = int(torch.where(trg[0] == 3)[0][0])\n",
    "        trg = trg[0][:eos_idx].unsqueeze(0)\n",
    "    else :\n",
    "        pass\n",
    "\n",
    "    # 번역\n",
    "    translation = [decoder_en[i] for i in trg.squeeze(0).tolist()]\n",
    "    print('모델예측 :',' '.join(translation[1:]))\n",
    "    \n",
    "\n",
    "    print('정답 :', en_item)\n",
    "    print('')\n",
    "    print('주의! 29,000개의 제한된 데이터로 학습을 수행했으므로 완벽한 예측이 불가능함.')\n",
    "\n",
    "\n",
    "\n",
    "test(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation 테스트\n",
    "\n",
    "* Transformer는 문제와 정답이 주어지면 병렬 연산이 가능함.\n",
    "\n",
    "* test 함수와의 차이를 비교할 수 있도록 validation을 포함하였음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "► Dataset is \"validation\"\n",
      "\n",
      "2번째 batch에 있는 0번째 문장 예측 결과 확인\n",
      "src :  Un chien noir dans l' herbe , tenant un objet en plastique blanc dans sa gueule .\n",
      "prd :  A black dog is in the grass with a microphone bench guitar in a mouth . <eos> <eos> <eos> . . . . . . . . . . . .\n",
      "trg :  A black dog standing in some grass holding a white plastic item in its mouth .\n",
      "\n",
      "\n",
      "4번째 batch에 있는 0번째 문장 예측 결과 확인\n",
      "src :  Le chien noir saute au-dessus de l' eau vers un frisbee flottant près d' un bateau .\n",
      "prd :  The black dog and on the beach with the body in . a boat . <eos> . . . . . . . <eos> . . . . . . . . .\n",
      "trg :  The black dog jumps above the water towards a Frisbee floating near a boat .\n",
      "\n",
      "\n",
      "6번째 batch에 있는 0번째 문장 예측 결과 확인\n",
      "src :  Un homme avec un badge est assis dans un fauteuil .\n",
      "prd :  A man with a beard purse on a holding in a field . <eos> . . . . . . . . . . . . <eos>\n",
      "trg :  A man with a name tag on is sitting in a chair .\n",
      "\n",
      "*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---\n",
      "Val_loss : 3.149\n",
      "*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---*---\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch_iter: Iterable):\n",
    "    \"\"\"\n",
    "    Data_loader에서 불러온 데이터를 가공하는 함수\n",
    "    토크나이징 => encoding => 시작 끝을 의미하는 spectial token(<bos>,<eos>) 추가 순으로 진행\n",
    "    \"\"\"\n",
    "    text_transform = {}\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        text_transform[ln] = utils.sequential_transforms(\n",
    "            token_transform[ln],  # 토크나이징\n",
    "            vocab_transform[ln],  # encoding\n",
    "            utils.tensor_transform, # BOS/EOS를 추가하고 텐서를 생성\n",
    "        )  \n",
    "        # sequential_transform, tensor_transform은 utils.py 참고\n",
    "    \n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch_iter:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n",
    "\n",
    "    # Pad 붙이기\n",
    "    PAD_IDX = 1\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch.T, tgt_batch.T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# token을 단어로 바꾸기 위한 dict 생성, vocab의 key와 value 위치 변경\n",
    "# 아래 helper 함수에서 활용됨.\n",
    "decoder_en = {v:k for k,v in vocab_transform['en'].get_stoi().items()}\n",
    "decoder_fr = {v:k for k,v in vocab_transform['fr'].get_stoi().items()}\n",
    "\n",
    "\n",
    "def helper_what_sen(src,trg,logits,i,c=100,sen_num=0) : \n",
    "    '''\n",
    "    문장이 제대로 학습되고 있는지를 확인하는 함수\n",
    "\n",
    "    src = encoding 된 source_sentence \n",
    "    trg = encoding 된 target_sentence\n",
    "    logits = 모델 예측값\n",
    "    i = 현재 batch 순서\n",
    "    c = 결과를 보여주는 단계, ex) c = 100이면 100,200,300... 번째 batch에서 결과를 보여줌\n",
    "    sen_num = batch 내 문장 중 몇 번째 문장을 추적할 것인지 설정\n",
    "    '''\n",
    "    if i % c == 0 and i != 0 :\n",
    "        src_sen = ' '.join([decoder_fr[i] for i in src.tolist()[sen_num] if decoder_fr[i][0] != '<' ])\n",
    "        trg_sen = ' '.join([decoder_en[i] for i in trg.tolist()[sen_num] if decoder_en[i][0] != '<' ])\n",
    "        prediction = logits.max(dim=-1, keepdim=False)[1][sen_num]\n",
    "        prd_sen = ' '.join([decoder_en[i] for i in prediction.tolist() if decoder_en[i] != '<' ])\n",
    "        '''\n",
    "        /*/* 모델의 예측 문장(prd_sen)을 구하는 방법 /*/* \n",
    "\n",
    "        n = batch size, trg_token_len = batch 내 문장의 최대 토큰 개수\n",
    "\n",
    "        모델 output(=logits)은 (n, trg_token_len, trg_vocab_len)의 3차원 텐서임.\n",
    "\n",
    "        1. 해당 텐서를 trg_vocab_len 차원의 기준으로 max를 하면 (n,trg_token_len)을 반환\n",
    "        2. tensor.max()의 수행 결과는 [최댓값,idx]를 반환함.\n",
    "        3. [1]을 넣어 idx를 선택, 그 결과는 (n, trg_token_len) 차원의 idx 반환\n",
    "        4. 원하는 문장 순서(sen_num)을 선택한 뒤 정수를 다시 단어로 decoding 수행\n",
    "        '''\n",
    "\n",
    "        print('')\n",
    "        print(f'{i}번째 batch에 있는 {sen_num}번째 문장 예측 결과 확인')\n",
    "        print('src : ',src_sen)\n",
    "        print('prd : ',prd_sen)\n",
    "        print('trg : ',trg_sen)\n",
    "        print('')\n",
    "\n",
    "    return None\n",
    "\n",
    "def evaluate(model):\n",
    "    #모델 평가모드 \n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    \n",
    "    # Load_Dataset\n",
    "    dataset= fr_to_en(set_type='validation')\n",
    "\n",
    "    # validation 데이터 불러오기\n",
    "    batch_size = 128\n",
    "    val_dataloader = DataLoader(dataset,batch_size,collate_fn=collate_fn)\n",
    "\n",
    "    for i,(src,tgt) in enumerate(val_dataloader) :\n",
    "        \n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        tgt_input = tgt[:,:-1]\n",
    "\n",
    "        logits = model(src,tgt_input)\n",
    "\n",
    "        helper_what_sen(src,tgt_input,logits,i,2,0) # 학습상태 확인\n",
    "\n",
    "        tgt_output = tgt[:,1:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt_output.reshape(-1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "##### Validation으로 테스트하기\n",
    "\n",
    "### 출력 문장수를 조정하고 싶으면 helper_what_sen의 parameter 수정\n",
    "\n",
    "val_loss = evaluate(model)\n",
    "\n",
    "print('*---'*20)\n",
    "print(f'Val_loss : {val_loss:.3f}')\n",
    "print('*---'*20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
